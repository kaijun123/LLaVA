{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to finetune the vision encoder for llava-med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/en/main_classes/image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r11kaijun/anaconda3/envs/llava-med/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/r11kaijun/anaconda3/envs/llava-med/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/r11kaijun/anaconda3/envs/llava-med/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-04 16:31:14,802] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r11kaijun/anaconda3/envs/llava-med/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/r11kaijun/anaconda3/envs/llava-med/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/r11kaijun/anaconda3/envs/llava-med/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.df = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Returns the absolute image file path and list of disease classifications'''\n",
    "        image = os.path.join(self.img_dir, self.df.at[idx, \"image_path\"])\n",
    "        labels = self.df.iloc[idx, 5:].values\n",
    "\n",
    "        labels_str = \"\"\n",
    "        for label in labels:\n",
    "            labels_str += str(label)\n",
    "            labels_str += \",\"\n",
    "\n",
    "        labels_str = labels_str[:-1]\n",
    "\n",
    "        return image, labels_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label_str(label_str_list):\n",
    "    '''\n",
    "    label_str_list: list of strings. Each string corresponds to each image\n",
    "    output: list of list of numbers. Each \"sublist\" contains the One-Hot encoded labels for each image\n",
    "    '''\n",
    "    label_list_list = []\n",
    "\n",
    "    for label_str in label_str_list:\n",
    "        # print(\"label_str:\", label_str)\n",
    "        label_list = []\n",
    "        for label in label_str.split(\",\"):\n",
    "            # print(\"label:\", label)\n",
    "            if label == '-1.0':\n",
    "                label_list.extend([1, 0, 0, 0])\n",
    "            elif label == '0.0':\n",
    "                label_list.extend([0, 1, 0, 0])\n",
    "            elif label == '1.0':\n",
    "                label_list.extend([0, 0, 1, 0])\n",
    "            elif label == '2.0':\n",
    "                label_list.extend([0, 0, 0, 1])\n",
    "            else:\n",
    "                raise ValueError(\"Invalid label: f{label}\")\n",
    "\n",
    "        label_list_list.append(label_list)\n",
    "            \n",
    "    return label_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomImageDataset(\n",
    "    annotations_file=\"/home/r11kaijun/MIMIC-CXR/processed_data/processed_mimic-cxr-2.0.0-chexpert_train.csv\",\n",
    "    img_dir=\"/home/r11kaijun/physionet.org/files/mimic-cxr-jpg/2.1.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "length of batch_image: 2\n",
      "batch_labels_str: ('1.0,-1.0,2.0,2.0,-1.0,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2.0,2.0', '2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,1.0,2.0,2.0,2.0,2.0,1.0')\n",
      "batch_labels: [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n",
      "length of batch_labels: 56\n"
     ]
    }
   ],
   "source": [
    "for i, (batch_image, batch_labels_str) in enumerate(train_dataloader):\n",
    "    print(\"i:\", i)\n",
    "    print(\"length of batch_image:\", len(batch_image))\n",
    "    print(\"batch_labels_str:\", batch_labels_str)\n",
    "    print(\"batch_labels:\", convert_label_str(batch_labels_str)[1])\n",
    "    print(\"length of batch_labels:\", len(convert_label_str(batch_labels_str)[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune the Vision Encoder\n",
    "- Note: Vision Encoder is in the Vision Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.mm_utils import process_images\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCLIPVisionTower(nn.Module):\n",
    "    def __init__(self, vision_tower, args, delay_load=False):\n",
    "        super().__init__()\n",
    "        self.vision_tower_name = vision_tower\n",
    "\n",
    "        self.select_layer = getattr(args, \"mm_vision_select_layer\", -2)\n",
    "        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n",
    "        print(\"self.select_feature:\", self.select_feature)\n",
    "        self.image_aspect_ratio = getattr(args, \"image_aspect_ratio\", \"pad\")\n",
    "        self.is_loaded = False\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if not delay_load:\n",
    "            self.load_model()\n",
    "        else:\n",
    "            self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\n",
    "            print(\"self.cfg_only:\", self.cfg_only)\n",
    "        self.is_loaded = True\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.is_loaded:\n",
    "            print(\n",
    "                \"{} is already loaded, `load_model` called again, skipping.\".format(\n",
    "                    self.vision_tower_name\n",
    "                )\n",
    "            )\n",
    "            return\n",
    "\n",
    "        print(\"self.vision_tower_name:\", self.vision_tower_name)\n",
    "        self.image_processor = CLIPImageProcessor.from_pretrained(\n",
    "            self.vision_tower_name\n",
    "        )\n",
    "        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name)\n",
    "\n",
    "        self.is_loaded = True\n",
    "\n",
    "    def feature_select(self, image_forward_outs):\n",
    "        \"\"\"Returns the CLS token and the patch embeddings (ie select_feature == 'cls_patch')\"\"\"\n",
    "        image_features = image_forward_outs.hidden_states[self.select_layer]\n",
    "        if self.select_feature == \"patch\":\n",
    "            image_features = image_features[:, 1:]\n",
    "            # TODO: Add additional processing methods to pool the results in each of the patch embeddings\n",
    "        elif self.select_feature == \"cls\":\n",
    "            image_features = image_features[:, 0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected select feature: {self.select_feature}\")\n",
    "        return image_features\n",
    "\n",
    "    # def get_tokens(self, select_feature, image_features):\n",
    "    #     \"\"\"\n",
    "    #     Function to obtain the CLS/ patch tokens after extracting the image features.\n",
    "    #     Can only be used when \"select_feature\" is \"cls_patch\"\n",
    "    #     \"\"\"\n",
    "    #     if select_feature == \"patch\":\n",
    "    #         image_features = image_features[:, 1:]\n",
    "    #     elif select_feature == \"cls\":\n",
    "    #         image_features = image_features[:, 0]\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Unexpected select feature: {self.select_feature}\")\n",
    "\n",
    "    #     return image_features\n",
    "\n",
    "    def preprocess(self, image_paths):\n",
    "        if not self.is_loaded:\n",
    "            raise ValueError(f\"Image processor is not loaded yet\")\n",
    "        images = []\n",
    "        for image_path in image_paths:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            images.append(image)\n",
    "\n",
    "        return process_images(images, self.image_processor, self.config)\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    def forward(self, images):\n",
    "        if type(images) is list:\n",
    "            image_features = []\n",
    "            for image in images:\n",
    "                image_forward_out = self.vision_tower(\n",
    "                    image.to(device=self.device, dtype=self.dtype).unsqueeze(0),\n",
    "                    output_hidden_states=True,\n",
    "                )\n",
    "                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n",
    "                image_features.append(image_feature)\n",
    "        else:\n",
    "            image_forward_outs = self.vision_tower(\n",
    "                images.to(device=self.device, dtype=self.dtype),\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    @property\n",
    "    def dummy_feature(self):\n",
    "        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.vision_tower.dtype\n",
    "\n",
    "    # @property\n",
    "    # def device(self):\n",
    "    #     return self.vision_tower.device\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        if self.is_loaded:\n",
    "            return self.vision_tower.config\n",
    "        else:\n",
    "            return self.cfg_only\n",
    "\n",
    "    @property\n",
    "    def hidden_size(self):\n",
    "        return self.config.hidden_size\n",
    "\n",
    "    @property\n",
    "    def num_patches(self):\n",
    "        return (self.config.image_size // self.config.patch_size) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"microsoft/llava-med-v1.5-mistral-7b\")\n",
    "    version: Optional[str] = field(default=\"mistral_instruct\")\n",
    "    freeze_backbone: bool = field(default=False)\n",
    "    tune_mm_mlp_adapter: bool = field(default=False)\n",
    "    vision_tower: Optional[str] = field(default=\"openai/clip-vit-large-patch14-336\")\n",
    "    mm_vision_select_layer: Optional[int] = field(default=-2)  # default to the last layer\n",
    "    pretrain_mm_mlp_adapter: Optional[str] = field(default=None)\n",
    "    mm_projector_type: Optional[str] = field(default='mlp2x_gelu')\n",
    "    mm_use_im_start_end: bool = field(default=False)\n",
    "    mm_use_im_patch_token: bool = field(default=True)\n",
    "    mm_patch_merge_type: Optional[str] = field(default='flat')\n",
    "    mm_vision_select_feature: Optional[str] = field(default=\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vision_tower(vision_tower_cfg, **kwargs):\n",
    "    vision_tower = getattr(vision_tower_cfg, 'mm_vision_tower', getattr(vision_tower_cfg, 'vision_tower', None))\n",
    "    print(\"vision_tower:\", vision_tower)\n",
    "    is_absolute_path_exists = os.path.exists(vision_tower)\n",
    "    if is_absolute_path_exists or vision_tower.startswith(\"openai\") or vision_tower.startswith(\"laion\"):\n",
    "        return CustomCLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_tower: openai/clip-vit-large-patch14-336\n",
      "self.select_feature: cls\n",
      "self.vision_tower_name: openai/clip-vit-large-patch14-336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r11kaijun/anaconda3/envs/llava-med/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/r11kaijun/anaconda3/envs/llava-med/lib/python3.10/site-packages/transformers/modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "vision_tower = build_vision_tower(ModelArguments())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (image_path, labels) in enumerate(train_dataloader):\n",
    "  print(\"image_path:\", image_path[0])\n",
    "  print(\"labels:\", labels)\n",
    "\n",
    "  image_tensors = vision_tower.preprocess(list(image_path))\n",
    "  # print(\"image_tensors:\", image_tensors)\n",
    "\n",
    "  image_features = vision_tower.forward(image_tensors)\n",
    "  print(\"image_features:\", image_features, image_features.shape)\n",
    "\n",
    "  # global average pooling\n",
    "  # pooled_features = image_features.mean(dim=1) \n",
    "  # print(\"pooled_features:\", pooled_features, pooled_features.shape)\n",
    "\n",
    "  break\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(577, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vision_tower.get_tokens(\"cls\", image_features)\n",
    "# vision_tower.get_tokens(\"patch\", image_features)\n",
    "vision_tower = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "# Freeze all layers\n",
    "for param in vision_tower.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last 4 layers\n",
    "num_layers = len(vision_tower.vision_model.encoder.layers)  # Total layers\n",
    "for param in vision_tower.vision_model.encoder.layers[-4:].parameters():\n",
    "    param.requires_grad = True  # Unfreeze last 4 layers\n",
    "\n",
    "vision_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDiseaseClassifier(nn.Module):\n",
    "    def __init__(self, input_neurons=1024, hidden_dim=1024, output_neurons=56):\n",
    "        super().__init__()\n",
    "        # MLP Classification Head\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_neurons, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, output_neurons),  # Output: (B, 56)\n",
    "        )\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        output = self.mlp(image_features)  # (B, 56)\n",
    "        # return output.view(-1, 14, 4)  # Reshape to (batch, 14 diseases, 4 states)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_clip_classifier(\n",
    "    vision_tower_instance: CustomCLIPVisionTower,\n",
    "    classifier: CLIPDiseaseClassifier,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    output_dir,\n",
    "    unfreeze_layers=4,\n",
    "    epochs=2,\n",
    "    lr=1e-4,\n",
    "):\n",
    "    # send the models to the gpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vision_tower_instance.to(device)\n",
    "    classifier.to(device)\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in vision_tower.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # unfreeze the layers that we want to finetune in the clip encoder\n",
    "    for param in vision_tower_instance.vision_tower.vision_model.encoder.layers[\n",
    "        -unfreeze_layers:\n",
    "    ].parameters():\n",
    "        param.requires_grad = True  # Unfreeze last 4 layers\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Multi-label classification loss\n",
    "    optimizer = optim.AdamW(\n",
    "        list(vision_tower_instance.parameters()) + list(classifier.parameters()),\n",
    "        lr=lr,\n",
    "    )\n",
    "\n",
    "    avg_train_loss_arr = []\n",
    "    avg_val_loss_arr = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch:\", epoch)\n",
    "        start_time = time.time()\n",
    "        vision_tower_instance.train()\n",
    "        classifier.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch, (image_paths, labels_str) in enumerate(train_loader):\n",
    "            print(\"batch:\", batch)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ground_truths = torch.Tensor(convert_label_str(labels_str)).to(device)\n",
    "            # print(\"ground_truths:\", ground_truths)\n",
    "\n",
    "            # preprocess images\n",
    "            images = vision_tower_instance.preprocess(image_paths).to(device)\n",
    "            image_features = vision_tower_instance.forward(images)\n",
    "\n",
    "            predicted_classes = classifier.forward(image_features)\n",
    "\n",
    "            loss = criterion(predicted_classes, ground_truths)  # Compute loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation Loop\n",
    "        vision_tower_instance.eval()\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (image_paths, labels_str) in enumerate(val_loader):\n",
    "                ground_truths = torch.Tensor(convert_label_str(labels_str)).to(device)\n",
    "                # print(\"ground_truths:\", ground_truths)\n",
    "\n",
    "                images = vision_tower_instance.preprocess(image_paths).to(device)\n",
    "                image_features = vision_tower_instance.forward(images)\n",
    "\n",
    "                predicted_classes = classifier.forward(image_features)\n",
    "\n",
    "                val_loss += criterion(predicted_classes, ground_truths).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {epoch_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        avg_train_loss_arr.append(avg_train_loss)\n",
    "        avg_val_loss_arr.append(avg_val_loss)\n",
    "\n",
    "    # save the models separately\n",
    "    torch.save(\n",
    "        vision_tower.state_dict(),\n",
    "        os.path.join(output_dir, f\"vision_tower-epoch-{epoch}-lr-{lr}.pth\"),\n",
    "    )\n",
    "    torch.save(\n",
    "        classifier.state_dict(),\n",
    "        os.path.join(output_dir, f\"vision_tower-epoch-{epoch}-lr-{lr}.pth\"),\n",
    "    )\n",
    "\n",
    "    return vision_tower, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_tower: openai/clip-vit-large-patch14-336\n",
      "self.select_feature: cls\n",
      "self.vision_tower_name: openai/clip-vit-large-patch14-336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r11kaijun/anaconda3/envs/llava-med/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/r11kaijun/anaconda3/envs/llava-med/lib/python3.10/site-packages/transformers/modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "batch: 0\n",
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 6\n",
      "batch: 7\n",
      "batch: 8\n",
      "batch: 9\n",
      "batch: 10\n",
      "batch: 11\n",
      "batch: 12\n",
      "batch: 13\n",
      "batch: 14\n",
      "batch: 15\n",
      "batch: 16\n",
      "batch: 17\n",
      "batch: 18\n",
      "batch: 19\n",
      "batch: 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(training_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m valdation_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(validation_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_clip_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_tower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvaldation_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 70\u001b[0m, in \u001b[0;36mtrain_clip_classifier\u001b[0;34m(vision_tower_instance, classifier, train_loader, val_loader, output_dir, unfreeze_layers, epochs, lr)\u001b[0m\n\u001b[1;32m     66\u001b[0m ground_truths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(convert_label_str(labels_str))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# print(\"ground_truths:\", ground_truths)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# preprocess images\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mvision_tower_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     71\u001b[0m image_features \u001b[38;5;241m=\u001b[39m vision_tower_instance\u001b[38;5;241m.\u001b[39mforward(images)\n\u001b[1;32m     73\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mforward(image_features)\n",
      "Cell \u001b[0;32mIn[43], line 68\u001b[0m, in \u001b[0;36mCustomCLIPVisionTower.preprocess\u001b[0;34m(self, image_paths)\u001b[0m\n\u001b[1;32m     66\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m image_paths:\n\u001b[0;32m---> 68\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m process_images(images, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m~/anaconda3/envs/llava-med/lib/python3.10/site-packages/PIL/Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llava-med/lib/python3.10/site-packages/PIL/ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vision_tower = build_vision_tower(ModelArguments())\n",
    "classifier = CLIPDiseaseClassifier()\n",
    "\n",
    "training_data = CustomImageDataset(\n",
    "    annotations_file=\"/home/r11kaijun/MIMIC-CXR/processed_data/processed_mimic-cxr-2.0.0-chexpert_train.csv\",\n",
    "    img_dir=\"/home/r11kaijun/physionet.org/files/mimic-cxr-jpg/2.1.0\",\n",
    ")\n",
    "validation_data = CustomImageDataset(\n",
    "    annotations_file=\"/home/r11kaijun/MIMIC-CXR/processed_data/processed_mimic-cxr-2.0.0-chexpert_validate.csv\",\n",
    "    img_dir=\"/home/r11kaijun/physionet.org/files/mimic-cxr-jpg/2.1.0\",\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=16, shuffle=True)\n",
    "valdation_dataloader = DataLoader(validation_data, batch_size=16, shuffle=True)\n",
    "\n",
    "train_clip_classifier(\n",
    "    vision_tower,\n",
    "    classifier,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=valdation_dataloader,\n",
    "    output_dir=\".\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth (using LLava-Med model and CLIP model as reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "print(image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_tower = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "print(vision_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "import torch\n",
    "from llava.model import LlavaMistralForCausalLM\n",
    "from llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", device=\"cuda\"):\n",
    "\n",
    "    kwargs = {}\n",
    "\n",
    "    if device != \"cuda\":\n",
    "        kwargs['device_map'] = {\"\": device}\n",
    "\n",
    "    if load_8bit:\n",
    "        kwargs['load_in_8bit'] = True\n",
    "    elif load_4bit:\n",
    "        kwargs['load_in_4bit'] = True\n",
    "        kwargs['quantization_config'] = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type='nf4'\n",
    "        )\n",
    "    else:\n",
    "        kwargs['torch_dtype'] = torch.float16\n",
    "    \n",
    "    if 'llava' in model_name.lower():\n",
    "        # Load LLaVA model\n",
    "            if 'mistral' in model_name.lower():\n",
    "                print(\"model_name:\", model_name)\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "                print(\"initialised tokenizer:\")\n",
    "                model = LlavaMistralForCausalLM.from_pretrained(\n",
    "                    model_path,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    use_flash_attention_2=False,\n",
    "                    **kwargs\n",
    "                )\n",
    "    else:\n",
    "        # Load language model\n",
    "        if model_base is not None:\n",
    "            # PEFT model\n",
    "            from peft import PeftModel\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\n",
    "            print(f\"Loading LoRA weights from {model_path}\")\n",
    "            model = PeftModel.from_pretrained(model, model_path)\n",
    "            print(f\"Merging weights\")\n",
    "            model = model.merge_and_unload()\n",
    "            print('Convert to FP16...')\n",
    "            model.to(torch.float16)\n",
    "        else:\n",
    "            use_fast = False\n",
    "            if 'mpt' in model_name.lower():\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n",
    "\n",
    "    image_processor = None\n",
    "\n",
    "    if 'llava' in model_name.lower(): # or 'mistral' in model_name.lower():\n",
    "        mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n",
    "        mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\n",
    "        if mm_use_im_patch_token:\n",
    "            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n",
    "        if mm_use_im_start_end:\n",
    "            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        vision_tower = model.get_vision_tower()\n",
    "        if not vision_tower.is_loaded:\n",
    "            vision_tower.load_model()\n",
    "        vision_tower.to(device=device, dtype=torch.float16)\n",
    "        model.model.mm_projector.to(device=device, dtype=torch.float16)\n",
    "        model.to(device=device, dtype=torch.float16)\n",
    "        image_processor = vision_tower.image_processor\n",
    "\n",
    "    if hasattr(model.config, \"max_sequence_length\"):\n",
    "        context_len = model.config.max_sequence_length\n",
    "    else:\n",
    "        context_len = 2048\n",
    "\n",
    "    return tokenizer, model, image_processor, context_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, model_base, model_name):\n",
    "    tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "        model_path=model_path,\n",
    "        model_base=model_base,\n",
    "        model_name=model_name,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    vision_tower = model.get_vision_tower()\n",
    "    if not vision_tower.is_loaded:\n",
    "        vision_tower.load_model()\n",
    "    vision_tower.to(device=\"cuda\", dtype=torch.float16)\n",
    "    model.model.mm_projector.to(device=\"cuda\", dtype=torch.float16)\n",
    "    model.to(device=\"cuda\", dtype=torch.float16)\n",
    "    image_processor = vision_tower.image_processor\n",
    "    if hasattr(model.config, \"max_sequence_length\"):\n",
    "        context_len = model.config.max_sequence_length\n",
    "    else:\n",
    "        context_len = 2048\n",
    "\n",
    "    return tokenizer, model, image_processor, context_len\n",
    "\n",
    "\n",
    "def load_base_model():\n",
    "    return load_model(\n",
    "        model_path=\"microsoft/llava-med-v1.5-mistral-7b\",\n",
    "        model_base=\"\",\n",
    "        model_name=\"microsoft/llava-med-v1.5-mistral-7b\",\n",
    "        # load_8bit=load_8bit,\n",
    "        # load_4bit=load_4bit,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model, image_processor, context_len = load_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"image_processor:\", image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLava-Med model overall architecture\n",
    "print(\"model:\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model:\", model.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLava-Med MLP layer\n",
    "print(model.model.mm_projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_vision_tower().vision_tower.vision_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_vision_tower().vision_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-med",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
